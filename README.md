# Team Tsinghua 2022 Software Tool

:warning: This is copied from https://gitlab.igem.org/2022/software-tools/tsinghua. I tried to clone it from gitlab.igem.org without success.

We have designed and completed two software tools, kmer2vec and Promoter_Transformer. With these two software tools, we would like to apply for the iGEM 2022 Software Tool Special Prize. 



## Description
The kmer2vec software is a novel alignment-free method based on word2vec training, which can be used for rapid multiple sequence comparisons, focusing on phylogenetic analysis and species clustering. With similar accuracy, kmer2vec can speed up the traditional multiple sequence alignment (MSA) method by about 10,000 times. Additionally, we also show the specific details of the method construction, which can be found in our paper “kmer2vec: A Novel Method for Comparing DNA Sequences by word2vec Embedding” published on the Journal of Computational Biology.

Promoter_Transformer is a new deep-learning model that can be used to predict the strength of prokaryotes promoters, which is the foundation of promoter computational-directed evolution. On the same dataset, the predictive performance of our model outperforms the predictive model in the paper “Synthetic promoter design in Escherichia coli based on a deep generative network” published on Nucleic Acids Research by 30%. Additionally, our software is validated by promoter mutation experiments with 90% effectiveness.

More details can be found in our Software page https://2022.igem.wiki/tsinghua/software.

## Installation
The environment needs:
Python 3.8
Pytorch
Gensim
Scikit-learn

## Usage
1.kmer2vec:
Here we show how to use the kmer2vec tool. 
Step 1 (Input dataset): Organize the open data in NCBI or your own sequencing data into a dataset in Fasta format (called 'seqdump.txt', with an end marker '!').
Step 2 (Use kmer2vec): Run the kmer2vec main program (called 'kmer2vec.py') to process the input dataset, and then kmer2vec will return three files. File 1 (called '5.txt') is the result of segmenting the input dataset, File 2 (called '5_txt_word2vec.model') is the saved word2vec neural network model, and File 3 (called 'results.txt') is the distance matrix generated by the multiple sequence comparison.
Step 3 (Visualization): The generated distance matrix file can be input into MEGA X to make a dendrogram reflecting the distance relationship.

Specifically, the 'sedump.txt' here is the sequences of SP-10 proteins. Additionally, we also provide the datasets of coronaviruses (called 'dataset_16cov.fasta'), influenza viruses (called 'Flu.fasta') and bacteria (called 'Bacteria.fasta').


2.Promoter_Transformer:
Using Promoter_Transformer to perform promoter directed evolution needs the following two steps.
Step 1 (Random mutation): First we should choose the promoter to be directed evolution and choose its evolution direction. For example, in our project we are going to evolve the promoter PNisA (called 'PnisA.txt') in the Nisin pathway to have a stronger strength. We will then use a program (called 'mutation.py') to randomly mutate the sequence of this promoter, and the number of mutated bases can be adjusted. We can then obtain a large number of mutated sequences obtained on the basis of this promoter sequence. 
Step 2 (Strength screening): After obtaining a large number of mutant sequences, we will input the large number of mutant sequences along with the training dataset (called 'all.npy' and 'all_y.npy';) into the Promoter_Transformer model. Afterwards, we will present the 10/20 sequences with the highest predicted strength based on the model feedback. In this way, we have completed the directed evolution of the promoter on the computer, which can be tested experimentally based on these sequence results. This step can use the Promoter_Transformer.py file in our code.

Specifically, the 'promoter.npy' is the promoter sequences in the training dataset and the 'gene_expression.npy' is the strength in the training dataset. Using the 'gene_expression.npy' to get the 'all_y.npy', the strength of mutated promoter can be regarded as zero. The 'mutation_prediction.xlsx' shows the prediction strength of the 50,000 mutated promoters and the 'Promoter_Transformer_training_preformance.txt' shows the loss and Pearson correlation of our trained model. 


## Contributing
We are open to contributions.



## Authors and acknowledgment
Software Author: Ruohan Ren
